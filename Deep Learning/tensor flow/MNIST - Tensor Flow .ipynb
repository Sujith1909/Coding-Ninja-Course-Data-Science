{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "sess = tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST has pictures which contains numbers from 0 to 9 and we have to classify the numbers by buildig a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.reshape(x_train,[60000,784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_784 = sess.run(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hotting the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, l_train), (x_test, l_test) = mnist.load_data()\n",
    "y_train = np.zeros((l_train.shape[0], l_train.max()+1), dtype=np.float32)\n",
    "y_train[np.arange(l_train.shape[0]), l_train] = 1\n",
    "y_test = np.zeros((l_test.shape[0], l_test.max()+1), dtype=np.float32)\n",
    "y_test[np.arange(l_test.shape[0]), l_test] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape # basically 60k images which are 28 by 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape # this has aldredy done one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x205b2ab9880>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[1001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot encoding is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.88487995 -0.45228162  0.49584103  0.36007383  0.2278034   0.16903675\n",
      "  1.7640127   0.34909463  0.49788085 -0.35177377 -0.25412196  1.2480556\n",
      "  1.1735003  -2.5057182  -0.143344    0.9155584  -0.5017987   0.14412406\n",
      " -0.4953779  -0.93733394  1.6014307  -1.2996519   0.16060661  0.08901265\n",
      "  0.7956936  -0.1918577  -0.25482517 -1.1626275  -0.08802119  0.99419576\n",
      " -0.9480447  -0.48342898 -0.726428   -1.1594645   0.8420311   0.79541993\n",
      " -0.9267085   0.5679269   0.4411769  -1.6810594   0.8804196   0.6701811\n",
      "  1.2898897  -1.5167042   0.34968218  0.11239199  0.1323117   1.3074721\n",
      "  0.6015164   1.4654812  -0.85336155  0.4495712  -2.2166014  -1.7434714\n",
      "  0.93213767 -1.0948665  -0.10266975  0.42074937  0.16662468 -1.2068566\n",
      " -1.568712    0.31988233  1.592421   -0.58699113  1.7575758  -0.3094304\n",
      "  0.28675407 -1.3526447  -0.12662886 -0.56598413 -0.6680466  -1.1602789\n",
      " -0.88260853  1.2512178   1.2879785  -0.5782505  -0.84440297 -1.0463419\n",
      " -2.0685532  -0.46985042 -0.16732751  0.7252232   1.5678712  -0.7553792\n",
      " -0.1302147   0.64163977  0.5468588   0.88052505 -1.1065582  -1.5769506\n",
      "  0.26710457 -0.3189916   0.8504087  -2.077728    0.38098556 -1.3908672\n",
      "  1.1480614   0.21381895  2.2297494  -0.4741969  -1.1809939  -0.6001367\n",
      "  0.24716604 -0.5260505   2.0241482   0.6161931   1.1298878  -0.69265646\n",
      "  0.39579692  1.0401496   0.12654509  0.7619877  -1.146516   -1.7841161\n",
      "  0.7341867   1.79649     0.26101637 -0.33227485 -0.17385276  0.52291614\n",
      " -0.5683103  -0.36175582 -0.14329597  1.0507869  -0.56405336  1.374887\n",
      "  0.23899177 -2.287103    0.45105484  1.7911748   1.9248409   1.3158069\n",
      "  1.4920245   0.4363004   0.76402926  0.16167445 -0.4636311  -0.12315512\n",
      "  0.21762447  2.088253   -2.1731818   0.02060137 -1.3105549  -1.8498335\n",
      "  0.63369495 -1.8820981  -0.99419886  1.3451406  -1.2160442  -0.2953155\n",
      "  1.5440857   2.3081696   0.3486386   0.37898487  0.7769232   1.3806576\n",
      "  0.6702296  -1.2266402   0.70529497 -0.01384529  0.89893705  1.3592284\n",
      "  0.68175143 -0.46692118 -1.5846275  -0.57155216  0.19275188 -0.03956182\n",
      "  0.29081845  2.3050802  -0.04353017  0.80526346 -0.6516396  -1.2078727\n",
      " -1.8717451  -0.09246763 -0.29092827  1.4197733   0.75921136  1.3937246\n",
      "  0.81636935  0.22358125 -1.9850177   0.36141384  1.7862647   0.40664896\n",
      "  0.86919653 -0.04414917 -0.84646153  0.7104573  -0.04512469 -2.288591\n",
      "  0.11940338  0.49748352  0.7622425   0.11964589  0.39745775 -0.5639687\n",
      "  2.1232464  -1.3248706  -1.4356313   0.4171535  -0.23480396 -1.2036363\n",
      "  0.2402411  -0.87440205  0.53362036 -0.625571    0.47018763  1.647706\n",
      " -1.6348395  -0.7473517   1.1299781   0.95083755 -0.932189    1.4640663\n",
      "  0.12382121 -0.9079539  -1.3050807   0.83413684  0.76056296 -0.8295219\n",
      "  0.585773   -0.32482722 -0.79801464  2.0687757   0.23773183 -0.7420864\n",
      "  0.0359568   1.2655609  -0.7541928  -0.8628637  -0.052372   -0.36520514\n",
      "  0.9619077  -0.11805126 -0.7940219   0.39870173  0.3732662   2.7014031\n",
      " -0.3412888  -0.1609808  -1.2072899  -1.9417179   0.29984018  0.6285263\n",
      "  1.0205362  -1.1758612  -0.70911855  1.9645222  -0.27454764 -1.4101887\n",
      "  0.04891942  0.3397834   0.17999475 -1.0598813  -1.8674331   0.52182925\n",
      " -0.14806914 -0.93700767 -2.704283   -0.95251995 -1.1624008   1.2443843\n",
      "  0.64030623 -0.1507117   1.0189881   0.61344594 -1.7595351  -1.138778\n",
      "  1.1368908  -0.6407243  -0.8719748  -1.7283398   0.8330477  -0.113941\n",
      "  0.96246046  0.7168892  -1.040318   -0.4807373   1.5599344   0.9401641\n",
      " -1.0967196  -1.4133664  -0.1388001  -0.49326026 -0.5101357   1.0287129\n",
      " -0.03837234  0.33776295 -1.9549621  -0.63566667 -0.08140086 -1.0520519\n",
      " -0.5050847   0.7861041  -2.108693    1.2686043   1.3703085  -0.38814658\n",
      "  0.23761861 -0.08402322 -0.4700479   0.56040955  0.14239094  0.9353202\n",
      " -0.3991154  -0.22518747  0.43369916 -1.0947338   0.20575874  0.08803272\n",
      " -0.51417404  0.78531665 -0.6852736   0.5810925   1.263439   -0.40655524\n",
      " -0.559721   -0.87983906  0.5411473   0.95587456 -1.5589617  -0.02205058\n",
      " -2.1568527  -0.42447525  0.44580904  1.0059589  -0.2276142  -0.34377542\n",
      "  0.512673    0.24615216 -0.989664    0.09592202 -0.30358186  1.5612831\n",
      " -1.2218522   0.9491565   0.36779976  2.0119312  -0.59719473  0.07757963\n",
      "  1.8387917  -1.5099776  -1.014658    0.47023955 -0.9027485   0.24494481\n",
      " -1.1373708   0.09852725  0.3258224  -2.6512082  -0.21086234 -0.63638103\n",
      " -0.27614513  0.7486682  -0.02415596 -0.58370197  0.9579472   0.4503695\n",
      " -0.16579378  0.76815873 -0.14547372 -0.85509497 -0.27398598  0.1142875\n",
      "  0.7654541   0.3690547   0.6438185  -0.32601288  0.6216131  -0.6128438\n",
      "  0.59975797 -0.1015375  -1.1817592   1.0647613  -0.5118284  -0.76667255\n",
      " -2.0761282   0.00359562 -2.5472622  -1.9382751  -0.30931577  1.1420904\n",
      "  1.2915692   0.9197256   0.09172016  1.540002    1.3386824   0.24238671\n",
      " -0.7892782   0.54297465  0.02060344  0.7469092  -0.711909    0.3537607\n",
      "  0.6870971  -0.38709775  0.57646865  0.21695986  0.49735066  1.0468816\n",
      "  1.4118499  -0.6428484   0.2186009   0.22155757  1.5762178  -0.6990311\n",
      " -0.5889657   1.0576636   0.80999744  0.69778746  0.0199559   0.99802727\n",
      "  1.084951    0.04383424  1.201053   -1.1740012  -1.1923716   1.5159965\n",
      " -0.30237755 -0.67389864  1.1419466  -0.50854164 -1.1541123   0.8445947\n",
      " -1.4533317   0.00686357 -0.01899187  0.11728472 -1.3227549   0.59867966\n",
      "  0.00455084 -0.4984921   1.2423543   0.87944645  0.29013473  0.7882809\n",
      " -1.053076   -0.3469249   0.0712794  -0.10517479 -0.4441649   0.15919311\n",
      "  0.994897    0.9829918  -0.525229    0.10577126  0.3199332  -1.8389302\n",
      "  0.2044684  -0.5892818  -0.10840248 -0.82745755 -0.39633232  0.7180909\n",
      "  1.1171744  -0.348733   -0.3667139  -0.93141043  0.02768668  1.8390571\n",
      " -0.22313218  0.92727566 -0.12795113  1.2101384  -0.5854854  -0.13670534\n",
      " -0.5098882  -0.06043383  0.17238435  0.643132   -0.89717954 -1.5800077\n",
      " -0.5010701   1.384604    0.92178184 -0.3785725   0.28206927  1.2125324\n",
      "  1.7173059  -0.55041313  2.5566127   1.2009097   0.5162268   0.6164956\n",
      "  0.744602   -1.0183108  -1.4845536   0.33783153  0.7547652  -0.8331536\n",
      " -0.19316366 -0.08865756 -0.39149418  0.6210906  -0.7698724  -1.6551645\n",
      " -0.4722067  -1.3302109   1.5837058   0.23920047  1.5292947   0.6296427\n",
      " -0.17770351  0.0769936  -1.5040536   0.12018673  0.07620426 -1.0873352\n",
      " -0.3130042   0.31367227  0.48185077  0.4488195   1.8192116  -0.78404105\n",
      "  0.5826362  -0.969989   -0.68699396  0.18213922 -0.91335696 -0.82515204\n",
      " -0.18474613 -0.49027205 -1.073792    0.5980854  -0.45782533  0.57829434\n",
      "  2.2234685  -1.4923005  -0.6257421  -1.2614472  -0.48246247  0.6501952\n",
      " -0.0750052   0.76287484  0.15448052 -0.69152737  0.58369166 -1.5509845\n",
      " -1.3628521   0.48817077  0.8637507   1.1929626  -1.2514734   0.7092727\n",
      " -1.3849952   1.6774176   0.49522832 -0.9959514  -0.30858138 -0.5610491\n",
      "  0.10269946  2.1179438  -1.0055293   0.32113513  1.29752     0.3756723\n",
      "  1.5739825  -1.2674263   0.41976526  0.3276351  -1.0806156   0.2805836\n",
      "  0.29926026 -1.2461213   1.1189137  -0.5608954   0.9292111   1.0163764\n",
      " -0.6119077  -1.0942278   0.07553995  1.8733579   1.8870047   0.44394812\n",
      "  0.43474445 -0.9226708   1.6111124   0.22158857  0.02639208  2.0315847\n",
      "  0.0444994   1.4898151  -0.3456062   0.3023319   0.84758115 -0.54165727\n",
      " -0.16764385  0.8699353   2.6518881   0.8030361   0.06658831 -0.22726272\n",
      " -0.95816535 -0.5993448  -0.9958703  -1.0829047  -0.13031252 -1.8809834\n",
      "  1.1760892  -1.0883389  -0.18909357 -0.06373028  0.8231208   1.5159786\n",
      " -3.0118039   1.6910866  -1.9079496   0.0871056  -1.5429178   0.53429216\n",
      "  2.1552458   0.77981997 -1.7484933  -0.6790642   0.7969166  -0.5843452\n",
      " -1.3064474   0.3384625   0.7284244   0.7459745  -1.3838116  -0.27869084\n",
      " -0.2863549   2.2808678  -1.0376139   0.09632106 -0.77881896  0.9984486\n",
      "  0.20384736 -0.38240027  0.6625084   0.62816393 -0.9813603  -0.13045098\n",
      " -0.03387256  1.4886959   1.2485212  -0.02822508  0.0432373   0.8458915\n",
      " -0.68070155  0.8167921  -0.6863068  -0.45243508 -0.47656038 -1.1233374\n",
      "  0.07170524  1.1511844   0.49029416  0.77809995  0.84320784  0.31726047\n",
      " -0.79067624 -2.3211133  -0.93631625  0.99330133 -0.9073241  -0.9442709\n",
      " -0.5798267  -0.46681452 -0.05231263  0.4718682   0.80065095 -1.1074799\n",
      " -0.9916249   0.54764897  1.6539795   0.6266439  -0.22234297 -1.3600012\n",
      "  1.0443625   0.09000369 -1.0722423  -0.22878677 -0.6737399   0.8578904\n",
      " -0.05611892  0.9157596   1.158708    0.18737268 -1.361116    0.25027454\n",
      "  0.30655032 -0.9622181   0.07743407 -0.18848622  0.652151   -0.78084475\n",
      "  0.52940786  0.6558868   0.7220487   0.3868412   1.1229651   0.79298574\n",
      "  1.3145471  -0.40579417  0.45347774 -2.1759503   0.48782933 -1.4662166\n",
      " -0.9871264   0.36382973  0.20282027 -0.69649017 -0.11811803  1.6127111\n",
      "  0.96574175 -0.75616485 -0.4139477  -0.51700723  0.3224186  -0.2994512\n",
      "  0.7657704  -1.3455038  -1.4449862   0.9201899   0.13810642 -0.947699\n",
      " -0.29390842  0.33376995  1.7316552  -0.24241534  1.2946568   0.8386222\n",
      "  0.20658827 -0.19304264  1.1824951   0.5005364   1.4741552  -1.2397304\n",
      "  0.3188163  -0.61087596  1.3530813   0.15801434 -0.08208486  0.62837636\n",
      "  0.5129414  -0.5221062  -1.2050574  -2.119676    0.9277232  -0.3436546\n",
      "  1.0855904   1.2982713  -1.4261576  -1.6420995  -0.65391004 -0.6649663\n",
      " -0.30760494 -0.07246072  0.5410623  -0.12220473 -0.4780799  -2.1938381\n",
      "  0.3423886   1.0127563   0.93574536  0.3324024   0.40599626 -0.19065207\n",
      " -2.0401714   0.08793789  0.04505856 -0.7597938  -0.11707401 -0.86233276\n",
      " -0.6231208  -0.40239134 -1.5826081   1.9262292   0.3867669  -0.8822624\n",
      "  0.2682357   0.6799556   0.7385788  -0.43269613 -0.7003805   0.01363796\n",
      "  1.1252745  -0.3205594  -0.03796994  0.6250982 ]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    print(tf.random_normal([784]).eval())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "->this is the archetecture for the input data for mnsit pitcure data \n",
    "->we are building a neural network with 2 hidden layers because our input is 784 we are choosing 256 as no of components in each hidden layer\n",
    "-> here we are building dictionaries for weights and biases \n",
    "-> h1 is for the weights from each layer to other layer input to h1 ,h1 to h2 , h2 to output\n",
    "-> same thing for biases we will have biases for each layer \n",
    "-> tf.random_normal([]) is the syntax for creating the object to create a normalized data \n",
    "-> this is not executed yet we have to do it later with a session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "n_classes = 10\n",
    "\n",
    "weights = {\n",
    "\n",
    "    'h1' : tf.Variable(tf.random_normal([n_input,n_hidden_1])),\n",
    "    'h2' : tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2,n_classes]))\n",
    "}\n",
    "biases ={ \n",
    "\n",
    "    'h1' : tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'h2' : tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of parametrs = 785 * 256 + 257 * 256 + 257 * 10\n",
    "# 200960 + 65792 + 2570\n",
    "# 269322"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation(x,weights,bias):\n",
    "    in_layer1 = tf.add(tf.matmul(x,weights['h1']),biases['h1']) # we are adding biases and the result of matrix multiplication of \n",
    "                                                                # weights and input\n",
    "    out_layer1 = tf.nn.relu(in_layer1) #and applying relu finction here\n",
    "    \n",
    "    in_layer2 = tf.add(tf.matmul(out_layer1,weights['h2']),biases['h2'])\n",
    "    out_layer2 = tf.nn.relu(in_layer2)\n",
    "    \n",
    "    output = tf.add(tf.matmul(out_layer2,weights['out']),biases['out'])\n",
    "    return output\n",
    "# now we have to figure out how do we optimize our weights here we are using randomly assigned weights \n",
    "# from the function tf.random_normal([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using these random weights lets find the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None,n_input])\n",
    "y = tf.placeholder(tf.int32,[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = forward_propogation(x,weights,biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.argmax(pred,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_labels = tf.argmax(y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.compat.v1.global_variables_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_eval  = sess.run(predictions,feed_dict = {x : x_train_784})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
