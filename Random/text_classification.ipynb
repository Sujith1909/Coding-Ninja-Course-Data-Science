{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups=fetch_20newsgroups() \n",
    "#instead of the link, I have fetched the data through sklearn itself just to reduce the amount of unnecessary work.\n",
    "stops=set(stopwords.words('english'))\n",
    "#these are the stopwords which we don't want to include in out vocabulary.\n",
    "punctuations=list(string.punctuation)\n",
    "#punctuation marks also should be removed from our vocabulary.\n",
    "stops.update(punctuations)\n",
    "#i have created a set of stop words and updated it with all the punctuation marks\n",
    "newsgroups.keys()\n",
    "#now we have all the unnecessary words and punctuation marks in our set \"stops\"\n",
    "#lets check what keys do we have in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#later on while debugging the code i realised that there are more stop words still residing there in these documents apart from\n",
    "#those i have already included.\n",
    "# so i copied more stop words from the internet and imported them using numpy.loadtxt() and updated my set of stop words.\n",
    "more_stops=np.loadtxt(\"datasets/stops.txt\", dtype=str, delimiter=\" \")\n",
    "stops.update(more_stops)\n",
    "len(stops)\n",
    "#now we have these many stop words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(newsgroups.data)\n",
    "#We have these many documents.(this is one of the reasons our algorithm is taking a considerable amount of time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents=newsgroups.data\n",
    "#all_documents contains all the \"X\" data in un-tokenized form.\n",
    "all_categories=newsgroups.target\n",
    "#all_cateories contain all the \"Y\" data.\n",
    "all_documents_modified=[word_tokenize(doc) for doc in all_documents]\n",
    "#i have modified the all_documents to a form in which we have list of list of words(tokenized data for each document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting these documents in train and test data.\n",
    "x_train, x_test, y_train, y_test=train_test_split(all_documents_modified, all_categories, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_words=[]\n",
    "#this variable is going to contain all the words from all our tokenized documents.\n",
    "for doc in x_train:\n",
    "    for word in doc:\n",
    "        if (word.lower() not in stops) and len(word)!=1 and len(word)!=2 and word[0]!=\"'\" and word!=\"n't\" and word[0]!=\".\":\n",
    "            #I dont't want to include words with length 1 and 2 in my vocabulary because these words are pretty much useless.\n",
    "            # and they might either be the stops which are not there in \"stops\" variable or some punctuation marks which are\n",
    "            # not there in \"punctuations\" variable. apart from the words of length 1 and 2 i have also removed some words\n",
    "            # separately because these words are most probably present in all the documents so they are not helping us much in\n",
    "            # classification.\n",
    "            all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_words)\n",
    "#lets check out the length of our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function accepts a list and returns a dictionary in which keys are the perticular words in the list and values are\n",
    "# the frequency of that element in that list.\n",
    "def freq_dict(all_words):\n",
    "    dic=dict()\n",
    "    #it iterates through all the elements in the list and increases the frequency by one if it encounters the same element again.\n",
    "    for word in all_words:\n",
    "        if word in dic.keys():\n",
    "            dic[word]+=1\n",
    "        else:\n",
    "            dic[word]=1\n",
    "    return dic\n",
    "\n",
    "dic=freq_dict(all_words)\n",
    "#Now in the \"dic\" variable we have the frequencies of all the elements of the list \"all words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#I have imported numpy because my plan is to get the elements in dic.keys() and dic.values() to two separate variables\n",
    "# namely \"freq\" and \"words\". and then i will arrange them in a decreasing fashion of frequency.\n",
    "freq=np.array([i for i in dic.values()])\n",
    "words=np.array([i for i in dic.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=words[np.argsort(freq)][::-1]\n",
    "freq=np.sort(freq)[::-1]\n",
    "#now i have sorted both the arrays and first arranged them in ascending order using np.sort() and np.argsort() and then i have\n",
    "# reversed the so obtained array to get a descending ordered array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell draws a frequency distribution graph for us to from where we can carefully observe the frequency trend of out list.\n",
    "import matplotlib.pyplot as plt\n",
    "#following are the limits for our graph.\n",
    "lower_limit=0    #default\n",
    "upper_limit=3000    #default\n",
    "difference_between_each_xtick=50  #default value. change these values to observe the graph in more depth\n",
    "#adjusting size of the graph for more clearity.\n",
    "plt.figure(figsize=(20, 7))\n",
    "#plotting\n",
    "plt.plot(np.arange(len(freq))[lower_limit:upper_limit], freq[lower_limit:upper_limit])\n",
    "#labelling\n",
    "plt.xlabel('Words(\"nth word\")--->')\n",
    "plt.ylabel(\"Frequency--->\")\n",
    "#adjusting the xticks\n",
    "plt.xticks(np.arange(lower_limit, upper_limit, 50), rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "#observe the following graph by changing the values of lower_limit, upper_limit and difference_between_each_xtick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=words[20:100000]\n",
    "#features variable contains all the top words which are most frequently used in all our documents. \n",
    "freq[20], freq[10000]\n",
    "# since there are 11314 documents I don't want the very high frequency words to get into my final features list as\n",
    "# they may be present in all the documents which will not help me much for classiication.\n",
    "# so i have choosen the words having frequency freq[20] to freq[9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function is the backbone of our Text Classification.\n",
    "# It takes the patameters x_train or x_test and the list of all features and converts it into a TEXT-CLASSIFICATION-FRIENDLY structure.\n",
    "# By text-classification-friendly, I mean it converts the data into a 2-D array which contains the frequency of that feature\n",
    "# in that perticular document. where rows are the documents and columns are the features.\n",
    "def data_modifier(x_data, features):\n",
    "    modified_data=np.zeros((len(x_data), len(features)))\n",
    "    #modified_data currently have a ZEROS numpy array whose shape is (len(x_data), len(features))\n",
    "    count=0\n",
    "    #Ignore this \"Count\" variable. i have created it just to keep an eye on the progress of our algorithm.\n",
    "    max_count=len(x_data)\n",
    "    for i in range(len(x_data)):\n",
    "        #looping over each and every row in the x_data\n",
    "        current_doc=x_data[i]\n",
    "        #current_doc contains the current document on which we are iterating.(As the name suggests obviously)\n",
    "        d=dict()\n",
    "        #this dictionary contains the frequency of all the elements in our current_doc.\n",
    "        for word in current_doc:\n",
    "            if word in d.keys():\n",
    "                d[word]+=1\n",
    "            else:\n",
    "                d[word]=1\n",
    "        #dictionary created\n",
    "        for j in range(len(features)):\n",
    "            #now for each feature in features we will insert the value of the dictionary for the corresponding. that is, \n",
    "            #the frequency of each feature in that current document.\n",
    "            if features[j] in d.keys():\n",
    "                modified_data[i][j]=d[features[j]]\n",
    "            else:\n",
    "                #if the current feature is not in the dictionary, it will remain 0 in the modified_data 2D matrix\n",
    "                continue\n",
    "        count+=1\n",
    "        print(\"progress: \", (count*100)/max_count, \"%\")\n",
    "        #This statement just prints the progress of our data_modifier function.\n",
    "    #finally I have returned the modified array.\n",
    "    return modified_data \n",
    "#this function takes some time to process because i have considered nearly 3000 features and apart from that we have so many\n",
    "#documents which contains so many words. so the algorithm has to go through each word in features list for each word in a perticular\n",
    "#document. and this needs to be done because I think there is probably no other way to create such a modified array from x_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_modified = data_modifier(x_train, features)\n",
    "#training and testing data has to be modified saparately because we cant use the testing data for training purpose\n",
    "#as both the Training and testing data must be having their own separate vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InBuilt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first trying out the inbuilt Multinomial naive bayes classifier.\n",
    "clf=MultinomialNB()\n",
    "clf.fit(x_train_modified, y_train)\n",
    "clf.score(x_test_modified, y_test)\n",
    "#it seems like a descent score to me beacause we haven't used real text classification algorithms like NLP and all.\n",
    "#If you want a the best possible accuracy, increase the number of features in the 13th cell to around 50,000.\n",
    "#you will get nearly 90 percent of accuracy.but i do not recommend it as the 49999th element will have the frequency of just 2.\n",
    "#and such a feature won't help us much for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit function takes the training data and gives you a dictionary whose keys are the different classes available to classify.\n",
    "# and for each key, value is another dictionary, whose keys are all the features available to us and value corresponding\n",
    "# to each key is sum of all the row of training data corresponding to the current class.(the original key or (say) primary key)\n",
    "def fit(x_train, y_train):\n",
    "    count=dict()\n",
    "    for i in range(20):\n",
    "        needed_docs=x_train[y_train==i]\n",
    "        #these are all the documents we need to work upon corresponding to the class i.\n",
    "        count[i]=dict()\n",
    "        #count is a dictionary whose each key is yet another dictionary.\n",
    "        count[i]['total']=0\n",
    "        #it will maintain the total number of words in class i.\n",
    "        for j in range(len(features)):\n",
    "            count[i][features[j]]=needed_docs[:, j].sum()\n",
    "            #it gives me-\"how many times jth feature is coming corresponding to class i\"\n",
    "            count[i]['total']+=count[i][features[j]]\n",
    "            #this additional key named \"total\" stores the sum of all the values of ith key which signifies the total number of\n",
    "            # words in class i.\n",
    "    #returning the dictionary at last.\n",
    "    return count\n",
    "\n",
    "#this \"probability\" function is another backbone of our naive bayes classifier.But before this function please ckeck out \n",
    "#predict function given below.\n",
    "#This \"Probability\" function takes takes the dictionary, which was originally returned by fit function, the current data point and the current class as its params.\n",
    "def probability(dictionary, x, current_class):\n",
    "    probas_for_each_word=[]\n",
    "    #my plan, here, is to store all the probabilities for each word in features which is available in the current document\n",
    "    # and then return the logarithmic sum of all these probabilities for that perticular document.\n",
    "    for i in range(len(x)):\n",
    "        #iterating through each feature.\n",
    "        if x[i]!=0:\n",
    "            #I am skipping those features whose value corresponding to the current document is 0, that is, they are not present\n",
    "            # in the that document.\n",
    "            numerator=dictionary[current_class][features[i]]\n",
    "            #it denotes that how many times that ith feature is occuring in \"current_class\"\n",
    "            #this is the numerator of our NON LOGARITHMIC PROBABILITY\n",
    "            denominator=dictionary[current_class]['total']\n",
    "            #it denotes that what is the total number of words in the current class.\n",
    "            #this is the denominator of our NON LOGARITHMIC PROBABILITY\n",
    "            proba=np.log((numerator+1)/(denominator+len(x)))\n",
    "            #i have created a variable \"proba\" which stores the logarithmic probability which also includes the laplace crrection.\n",
    "            #here \"1\" with the numerator and len(x) with denominator denotes the laplace correction.\n",
    "            probas_for_each_word.append(proba)\n",
    "            #finally i appended \"proba\" to the probas_for_each_word array.\n",
    "    #returning the logarithmic sum of all the probabilities. (which can be treated as multiplication of all the probabilities,\n",
    "    #but multiplying the probabilities simply won't be a good practice as it may nullify the extreamily small probabilities)\n",
    "    return sum(probas_for_each_word)\n",
    "\n",
    "#The predict_single function takes a perticular data point \"x\" from \"predict\" function and a dictionary which was obtained\n",
    "#originally from fit function. Before going through this function, you may like to check out the \"predict\" function.\n",
    "def predict_single(dic, x):\n",
    "    classes = dictionary.keys()\n",
    "    #this variable \"classes\" maintains all the possible different classes available to us.\n",
    "    best_p = -1000\n",
    "    best_class = -1\n",
    "    #initiated best_p and best_class variables with negative values because we want to consider maximum valued probabilities at\n",
    "    #the end. In the first run itself they will be replaced by positive values. \"best_p\" variable denotes the best probability\n",
    "    #uptill now. and as the name suggests, best_class denotes the best possible class uptill now.\n",
    "    first_run = True\n",
    "    #this boolean variable is taken keeping initial negative values in mind\n",
    "    for current_class in classes:\n",
    "        #iterating through each and every class in all possible classes.\n",
    "        p_current_class = probability(dic, x, current_class)\n",
    "        #p_current_class denotes the probability of current class.\n",
    "        if (first_run or p_current_class > best_p):\n",
    "            #if the probability of current class is better than the best probability then i will update the best probability and\n",
    "            # best_class to probability of current class and the current class respectively.\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "            #updated\n",
    "        first_run = False\n",
    "    #returning the best predicted class for the current data point\n",
    "    return best_class\n",
    "\n",
    "#The predict function takes testing data and a dictionary as its input, where the dictionary is the one returned from fit function.\n",
    "def predict(x_test, dic):\n",
    "    y_pred=[]\n",
    "    #y_pred maintains all the predicted classes for provided testing data(In modified format---> see the \"data_modifier\" function).\n",
    "    for doc in x_test:\n",
    "        #iterating each document in testing data.\n",
    "        y_pred.append(predict_single(dic, doc))\n",
    "        #for each testing data point, we will append the value of predicted class to y_pred. and we will get that value from\n",
    "        #predict_single function\n",
    "    #returning the predicted data finally\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=fit(x_train_modified, y_train)\n",
    "#calling the fit function for the modified data.\n",
    "y_predicted=predict(x_test_modified, dictionary)\n",
    "#storing the predicted values.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#printing the confusion matrix for our own naive bayes classifier.\n",
    "#here i am manually printing the confusion matrix for a more clear view.\n",
    "for i in confusion_matrix(y_true=y_test, y_pred=y_predicted):\n",
    "    for j in i:\n",
    "        print(j, end=\"    \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "#printing the classification report for our own naive bayes classifier.\n",
    "print(classification_report(y_true=y_test, y_pred=y_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
